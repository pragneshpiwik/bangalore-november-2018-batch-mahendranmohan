{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SeqNLP_Project1_Questions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xT7MKZuMRaCg"
      },
      "source": [
        "# Sentiment Classification\n",
        "\n",
        "\n",
        "### Generate Word Embeddings and retrieve outputs of each layer with Keras based on Classification task\n",
        "\n",
        "Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n",
        "\n",
        "It is a distributed representation for text that is perhaps one of the key breakthroughs for the impressive performance of deep learning methods on challenging natural language processing problems.\n",
        "\n",
        "We willl use the imdb dataset to learn word embeddings as we train our dataset. This dataset contains 25,000 movie reviews from IMDB, labeled with sentiment (positive or negative). \n",
        "\n",
        "\n",
        "\n",
        "### Dataset\n",
        "\n",
        "`from keras.datasets import imdb`\n",
        "\n",
        "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most frequent word. Use the first 20 words from each review to speed up training, using a max vocab size of 10,000.\n",
        "\n",
        "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "\n",
        "\n",
        "### Aim\n",
        "\n",
        "1. Import test and train data  \n",
        "2. Import the labels ( train and test) \n",
        "3. Get the word index and then Create key value pair for word and word_id. (12.5 points)\n",
        "4. Build a Sequential Model using Keras for Sentiment Classification task. (10 points)\n",
        "5. Report the Accuracy of the model. (5 points)  \n",
        "6. Retrive the output of each layer in keras for a given single test sample from the trained model you built. (2.5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Wq4RCyyPSYRp"
      },
      "source": [
        "#### Usage:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QBoHPgjssNh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "139de912-4e67-4ca8-f361-475c41932d47"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfRfg0OCsxVN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir('/content/drive/My Drive/Colab Notebooks/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NGCtiXUhSWss",
        "colab": {}
      },
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "vocab_size = 10000 #vocab size\n",
        "\n",
        "import numpy as np\n",
        "# save np.load\n",
        "np_load_old = np.load\n",
        "# modify the default parameters of np.load\n",
        "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
        "# This is to call load_data with allow_pickle implicitly set to true\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size) # vocab_size is no.of words to consider from the dataset, ordering based on frequency.\n",
        "\n",
        "\n",
        "# restore np.load for future normal usage\n",
        "np.load = np_load_old\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihvVb9FGjhdz",
        "colab_type": "code",
        "outputId": "84d69bb4-63c4-4c67-d187-d93120339c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "print(x_train.shape,y_train.shape, x_test.shape, y_test.shape)\n",
        "print(x_train[3])\n",
        "print(y_train[3])\n",
        "print(\"Max val: \", max([max(sequence) for sequence in x_train]))\n",
        "print(\"Max len: \", max([len(sequence) for sequence in x_train]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,) (25000,) (25000,) (25000,)\n",
            "[1, 4, 2, 2, 33, 2804, 4, 2040, 432, 111, 153, 103, 4, 1494, 13, 70, 131, 67, 11, 61, 2, 744, 35, 3715, 761, 61, 5766, 452, 9214, 4, 985, 7, 2, 59, 166, 4, 105, 216, 1239, 41, 1797, 9, 15, 7, 35, 744, 2413, 31, 8, 4, 687, 23, 4, 2, 7339, 6, 3693, 42, 38, 39, 121, 59, 456, 10, 10, 7, 265, 12, 575, 111, 153, 159, 59, 16, 1447, 21, 25, 586, 482, 39, 4, 96, 59, 716, 12, 4, 172, 65, 9, 579, 11, 6004, 4, 1615, 5, 2, 7, 5168, 17, 13, 7064, 12, 19, 6, 464, 31, 314, 11, 2, 6, 719, 605, 11, 8, 202, 27, 310, 4, 3772, 3501, 8, 2722, 58, 10, 10, 537, 2116, 180, 40, 14, 413, 173, 7, 263, 112, 37, 152, 377, 4, 537, 263, 846, 579, 178, 54, 75, 71, 476, 36, 413, 263, 2504, 182, 5, 17, 75, 2306, 922, 36, 279, 131, 2895, 17, 2867, 42, 17, 35, 921, 2, 192, 5, 1219, 3890, 19, 2, 217, 4122, 1710, 537, 2, 1236, 5, 736, 10, 10, 61, 403, 9, 2, 40, 61, 4494, 5, 27, 4494, 159, 90, 263, 2311, 4319, 309, 8, 178, 5, 82, 4319, 4, 65, 15, 9225, 145, 143, 5122, 12, 7039, 537, 746, 537, 537, 15, 7979, 4, 2, 594, 7, 5168, 94, 9096, 3987, 2, 11, 2, 4, 538, 7, 1795, 246, 2, 9, 2, 11, 635, 14, 9, 51, 408, 12, 94, 318, 1382, 12, 47, 6, 2683, 936, 5, 6307, 2, 19, 49, 7, 4, 1885, 2, 1118, 25, 80, 126, 842, 10, 10, 2, 2, 4726, 27, 4494, 11, 1550, 3633, 159, 27, 341, 29, 2733, 19, 4185, 173, 7, 90, 2, 8, 30, 11, 4, 1784, 86, 1117, 8, 3261, 46, 11, 2, 21, 29, 9, 2841, 23, 4, 1010, 2, 793, 6, 2, 1386, 1830, 10, 10, 246, 50, 9, 6, 2750, 1944, 746, 90, 29, 2, 8, 124, 4, 882, 4, 882, 496, 27, 2, 2213, 537, 121, 127, 1219, 130, 5, 29, 494, 8, 124, 4, 882, 496, 4, 341, 7, 27, 846, 10, 10, 29, 9, 1906, 8, 97, 6, 236, 2, 1311, 8, 4, 2, 7, 31, 7, 2, 91, 2, 3987, 70, 4, 882, 30, 579, 42, 9, 12, 32, 11, 537, 10, 10, 11, 14, 65, 44, 537, 75, 2, 1775, 3353, 2, 1846, 4, 2, 7, 154, 5, 4, 518, 53, 2, 2, 7, 3211, 882, 11, 399, 38, 75, 257, 3807, 19, 2, 17, 29, 456, 4, 65, 7, 27, 205, 113, 10, 10, 2, 4, 2, 2, 9, 242, 4, 91, 1202, 2, 5, 2070, 307, 22, 7, 5168, 126, 93, 40, 2, 13, 188, 1076, 3222, 19, 4, 2, 7, 2348, 537, 23, 53, 537, 21, 82, 40, 2, 13, 2, 14, 280, 13, 219, 4, 2, 431, 758, 859, 4, 953, 1052, 2, 7, 5991, 5, 94, 40, 25, 238, 60, 2, 4, 2, 804, 2, 7, 4, 9941, 132, 8, 67, 6, 22, 15, 9, 283, 8, 5168, 14, 31, 9, 242, 955, 48, 25, 279, 2, 23, 12, 1685, 195, 25, 238, 60, 796, 2, 4, 671, 7, 2804, 5, 4, 559, 154, 888, 7, 726, 50, 26, 49, 7008, 15, 566, 30, 579, 21, 64, 2574]\n",
            "1\n",
            "Max val:  9999\n",
            "Max len:  2494\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zjlhwemlj-x",
        "colab_type": "code",
        "outputId": "62537c4c-cd9a-4d45-861d-f6d54c54adae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "vocabulary = imdb.get_word_index()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4AFq51ml0l_",
        "colab_type": "code",
        "outputId": "2c8c8f26-f08c-448c-c3c6-6dde70e5c424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# A dictionary mapping words to an integer index\n",
        "#vocabulary = imdb.get_word_index()\n",
        "\n",
        "# The first indices are reserved\n",
        "vocabulary = {k:(v+3) for k,v in vocabulary.items()} \n",
        "vocabulary[\"<PAD>\"] = 0\n",
        "# See how integer 1 appears first in the review above. \n",
        "vocabulary[\"<START>\"] = 1\n",
        "vocabulary[\"<UNK>\"] = 2  # unknown\n",
        "vocabulary[\"<UNUSED>\"] = 3\n",
        "\n",
        "# reversing the vocabulary. \n",
        "# in the index, the key is an integer, \n",
        "# and the value is the corresponding word.\n",
        "index = dict([(value, key) for (key, value) in vocabulary.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    '''converts encoded text to human readable form.\n",
        "    each integer in the text is looked up in the index, and \n",
        "    replaced by the corresponding word.\n",
        "    '''\n",
        "    return ' '.join([index.get(i, '?') for i in text])\n",
        "\n",
        "decode_review(x_train[3])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"<START> the <UNK> <UNK> at storytelling the traditional sort many years after the event i can still see in my <UNK> eye an elderly lady my friend's mother retelling the battle of <UNK> she makes the characters come alive her passion is that of an eye witness one to the events on the <UNK> heath a mile or so from where she lives br br of course it happened many years before she was born but you wouldn't guess from the way she tells it the same story is told in bars the length and <UNK> of scotland as i discussed it with a friend one night in <UNK> a local cut in to give his version the discussion continued to closing time br br stories passed down like this become part of our being who doesn't remember the stories our parents told us when we were children they become our invisible world and as we grow older they maybe still serve as inspiration or as an emotional <UNK> fact and fiction blend with <UNK> role models warning stories <UNK> magic and mystery br br my name is <UNK> like my grandfather and his grandfather before him our protagonist introduces himself to us and also introduces the story that stretches back through generations it produces stories within stories stories that evoke the <UNK> wonder of scotland its rugged mountains <UNK> in <UNK> the stuff of legend yet <UNK> is <UNK> in reality this is what gives it its special charm it has a rough beauty and authenticity <UNK> with some of the finest <UNK> singing you will ever hear br br <UNK> <UNK> visits his grandfather in hospital shortly before his death he burns with frustration part of him <UNK> to be in the twenty first century to hang out in <UNK> but he is raised on the western <UNK> among a <UNK> speaking community br br yet there is a deeper conflict within him he <UNK> to know the truth the truth behind his <UNK> ancient stories where does fiction end and he wants to know the truth behind the death of his parents br br he is pulled to make a last <UNK> journey to the <UNK> of one of <UNK> most <UNK> mountains can the truth be told or is it all in stories br br in this story about stories we <UNK> bloody battles <UNK> lovers the <UNK> of old and the sometimes more <UNK> <UNK> of accepted truth in doing so we each connect with <UNK> as he lives the story of his own life br br <UNK> the <UNK> <UNK> is probably the most honest <UNK> and genuinely beautiful film of scotland ever made like <UNK> i got slightly annoyed with the <UNK> of hanging stories on more stories but also like <UNK> i <UNK> this once i saw the <UNK> picture ' forget the box office <UNK> of braveheart and its like you might even <UNK> the <UNK> famous <UNK> of the wicker man to see a film that is true to scotland this one is probably unique if you maybe <UNK> on it deeply enough you might even re <UNK> the power of storytelling and the age old question of whether there are some truths that cannot be told but only experienced\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCPC_WN-eCyw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from keras.preprocessing.sequence import pad_sequences\n",
        "import keras\n",
        "vocab_size = 10000 #vocab size\n",
        "maxlen = 300  #number of word used from each review"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0g381XzeCyz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get the input to be of fixed shape\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(x_train, value=vocabulary[\"<PAD>\"], padding='post', maxlen=maxlen)\n",
        "\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(x_test, value=vocabulary[\"<PAD>\"], padding='post', maxlen=maxlen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy6n-uM2eCy2",
        "colab_type": "code",
        "outputId": "eb6b8514-3def-4e83-93d6-36c53834a2f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(X_train.shape, y_train.shape,  X_test.shape, y_test.shape)\n",
        "\n",
        "print(\"Max val: \", max([max(sequence) for sequence in X_train]))\n",
        "print(\"Max len: \", max([len(sequence) for sequence in X_train]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000, 300) (25000,) (25000, 300) (25000,)\n",
            "Max val:  9999\n",
            "Max len:  300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dybtUgUReCy8",
        "colab_type": "text"
      },
      "source": [
        "## Build Keras Embedding Layer Model\n",
        "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
        "\n",
        "* The embedding layer can be used at the start of a larger deep learning model. \n",
        "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
        "* Use the embedding layer to train our own word2vec models.\n",
        "\n",
        "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5OLM4eBeCy9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "outputId": "682d1d7a-d3a1-4263-da08-0223743a2e76"
      },
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "import os\n",
        "# create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 32, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(250, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0804 17:38:59.774760 140415576274816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0804 17:38:59.788403 140415576274816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0804 17:38:59.842681 140415576274816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0804 17:38:59.861655 140415576274816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0804 17:38:59.866667 140415576274816 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 300, 32)           320000    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9600)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 250)               2400250   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 2,720,501\n",
            "Trainable params: 2,720,501\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxNDNhrseCzA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "b209d2ce-c5d8-4996-911d-a0b3c54df8bf"
      },
      "source": [
        "# Fit the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32, verbose=2)\n",
        "# Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0804 17:39:05.920985 140415576274816 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/5\n",
            " - 8s - loss: 0.4058 - acc: 0.8002 - val_loss: 0.3932 - val_acc: 0.8297\n",
            "Epoch 2/5\n",
            " - 4s - loss: 0.1073 - acc: 0.9612 - val_loss: 0.4241 - val_acc: 0.8424\n",
            "Epoch 3/5\n",
            " - 4s - loss: 0.0175 - acc: 0.9944 - val_loss: 0.6816 - val_acc: 0.8426\n",
            "Epoch 4/5\n",
            " - 4s - loss: 0.0052 - acc: 0.9985 - val_loss: 0.8235 - val_acc: 0.8445\n",
            "Epoch 5/5\n",
            " - 4s - loss: 0.0189 - acc: 0.9938 - val_loss: 0.8060 - val_acc: 0.8351\n",
            "Accuracy: 83.51%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3CSVVPPeCzD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "84663ae7-6040-4558-98e2-674042d6dc6d"
      },
      "source": [
        "# save the model to file\n",
        "model.save('SeqNLP_Project1_imdb_model.h5')\n",
        "!ls -lrt SeqNLP_Project1_imdb_model.h5"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw------- 1 root root 32672400 Aug  4 17:39 SeqNLP_Project1_imdb_model.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Igq8Qm8GeCzG",
        "colab_type": "text"
      },
      "source": [
        "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AqOnLa2eCzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "outputId": "17bd0d14-e892-49e5-b0ed-71001ce9c415"
      },
      "source": [
        "model.fit(X_train, y_train, batch_size=32, epochs=10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 4s 153us/step - loss: 0.0190 - acc: 0.9931\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 4s 151us/step - loss: 0.0042 - acc: 0.9988\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 4s 154us/step - loss: 4.3270e-04 - acc: 1.0000\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 4s 148us/step - loss: 2.9819e-05 - acc: 1.0000\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 4s 150us/step - loss: 1.4142e-05 - acc: 1.0000\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 4s 149us/step - loss: 9.1127e-06 - acc: 1.0000\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 4s 147us/step - loss: 6.0205e-06 - acc: 1.0000\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 4s 148us/step - loss: 4.0299e-06 - acc: 1.0000\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 4s 149us/step - loss: 2.7181e-06 - acc: 1.0000\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 4s 149us/step - loss: 1.8793e-06 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb4b8fe6978>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dUDSg7VeCzM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "45a5b8dd-7c7d-4d23-bd7c-9d8395cafd96"
      },
      "source": [
        "score, acc = model.evaluate(X_test, y_test,\n",
        "                            batch_size=32)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 1s 41us/step\n",
            "Test score: 1.2551728568255902\n",
            "Test accuracy: 0.84288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tskt_1npeCzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}